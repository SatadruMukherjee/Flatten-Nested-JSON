{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Best Code.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512
        },
        "id": "pfgUIIm1FJ80",
        "outputId": "26a24c5f-414b-4ba0-8c7b-241bbe4e2795"
      },
      "source": [
        "!apt-get update\r\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\r\n",
        "!wget -q htt  https://archive.apache.org/dist/spark/spark-2.3.1/spark-2.3.1-bin-hadoop2.7.tgz\r\n",
        "!tar xf spark-2.3.1-bin-hadoop2.7.tgz\r\n",
        "!pip install -q findspark\r\n",
        "\r\n",
        "import os\r\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\r\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.3.1-bin-hadoop2.7\"\r\n",
        "\r\n",
        "import findspark\r\n",
        "findspark.init()\r\n",
        "from pyspark.sql import SparkSession\r\n",
        "spark = SparkSession.builder.getOrCreate()\r\n",
        "spark"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.88.142)] [Connecting to security.u\r0% [Connecting to archive.ubuntu.com (91.189.88.142)] [Connecting to security.u\r0% [1 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (91.189.88.142)\r                                                                               \rIgn:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "\r0% [1 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (91.189.88.142)\r                                                                               \rIgn:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "\r0% [1 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (91.189.88.142)\r                                                                               \rHit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:6 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:8 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:11 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Hit:14 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:15 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,716 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,149 kB]\n",
            "Get:17 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [878 kB]\n",
            "Fetched 5,015 kB in 4s (1,423 kB/s)\n",
            "Reading package lists... Done\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://a4981ecb13f0:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v2.3.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f6767cdccf8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iusVxgCMXtOf"
      },
      "source": [
        "# **Read the 3 different files**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f56cDRIjFOpx",
        "outputId": "8f37526c-5387-4317-d108-1fd027f591ee"
      },
      "source": [
        "df1=spark.read.json(\"/content/1.json\", multiLine=True);\r\n",
        "df2=spark.read.json(\"/content/2.json\", multiLine=True);\r\n",
        "df3=spark.read.json(\"/content/3.json\", multiLine=True);\r\n",
        "df1.printSchema()\r\n",
        "df2.printSchema()\r\n",
        "df3.printSchema()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- A: struct (nullable = true)\n",
            " |    |-- B: long (nullable = true)\n",
            "\n",
            "root\n",
            " |-- A: struct (nullable = true)\n",
            " |    |-- B: long (nullable = true)\n",
            " |-- C: long (nullable = true)\n",
            "\n",
            "root\n",
            " |-- A: struct (nullable = true)\n",
            " |    |-- B: long (nullable = true)\n",
            " |    |-- D: long (nullable = true)\n",
            " |-- C: long (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9kuY84sXxcO"
      },
      "source": [
        "# Importing the functions "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkT2FJ_tTSV0"
      },
      "source": [
        "from pyspark.sql.functions import *\r\n",
        "from pyspark.sql.types import *"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JXzYZkJTcbo"
      },
      "source": [
        "def read_nested_json(df):\r\n",
        "    column_list = []\r\n",
        "    for column_name in df.schema.names:\r\n",
        "        if isinstance(df.schema[column_name].dataType, ArrayType):\r\n",
        "            df = df.withColumn(column_name,explode(column_name))\r\n",
        "            column_list.append(column_name)\r\n",
        "        elif isinstance(df.schema[column_name].dataType, StructType):\r\n",
        "            for field in df.schema[column_name].dataType.fields:\r\n",
        "                column_list.append(col(column_name + \".\" + field.name).alias(column_name + \"_\" + field.name))\r\n",
        "        else:\r\n",
        "            column_list.append(column_name)\r\n",
        "    df = df.select(column_list)\r\n",
        "    return df"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2yhJVHcTGwf"
      },
      "source": [
        "def flatten(df):\r\n",
        "  read_nested_json_flag = True\r\n",
        "  while read_nested_json_flag:\r\n",
        "    df = read_nested_json(df);\r\n",
        "    read_nested_json_flag = False\r\n",
        "    for column_name in df.schema.names:\r\n",
        "      if isinstance(df.schema[column_name].dataType, ArrayType):\r\n",
        "        read_nested_json_flag = True\r\n",
        "      elif isinstance(df.schema[column_name].dataType, StructType):\r\n",
        "        read_nested_json_flag = True;\r\n",
        "  return df;"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcAy13SLX7kQ"
      },
      "source": [
        "# **Call the function to flatten the files**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxy7V7RcTp8k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0e90313-e322-461c-d2da-5ebbb410037b"
      },
      "source": [
        "df1=flatten(df1);\r\n",
        "df1.show();\r\n",
        "df2=flatten(df2);\r\n",
        "df2.show();\r\n",
        "df3=flatten(df3);\r\n",
        "df3.show();"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+\n",
            "|A_B|\n",
            "+---+\n",
            "|  1|\n",
            "+---+\n",
            "\n",
            "+---+---+\n",
            "|A_B|  C|\n",
            "+---+---+\n",
            "|  1|  2|\n",
            "+---+---+\n",
            "\n",
            "+---+---+---+\n",
            "|A_B|A_D|  C|\n",
            "+---+---+---+\n",
            "|  1|  3|  2|\n",
            "+---+---+---+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qcYhhHFcqOr5"
      },
      "source": [
        "monitor=spark.sparkContext.accumulator(0);"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhPxSHeDYGkf"
      },
      "source": [
        "# **Create a directory to store the files which are not following the standard schema**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QaKU8M63p4z4"
      },
      "source": [
        "mkdir '/content/Corrupted Files Storage'"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-DfNbG6YBFD"
      },
      "source": [
        "# Harmonize the Schema "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKF8HUD-pfJ-"
      },
      "source": [
        "def harmonize_schemas(standard_schema, source_file):\r\n",
        "    left_types = {f.name: f.dataType for f in standard_schema}\r\n",
        "    right_types = {f.name: f.dataType for f in source_file.schema}\r\n",
        "    left_fields = set((f.name, f.dataType,f.nullable) for f in standard_schema)\r\n",
        "    right_fields = set((f.name, f.dataType,f.nullable) for f in source_file.schema)\r\n",
        "    global monitor;\r\n",
        "    monitor.add(1);\r\n",
        "    file_number=monitor.value;\r\n",
        "    for l_name, l_type ,l_nullable in left_fields.difference(right_fields):\r\n",
        "      if (l_name in right_types):\r\n",
        "        r_type=right_types[l_name];\r\n",
        "        if (l_type!=r_type):\r\n",
        "          source_file = source_file.withColumn(l_name, source_file[l_name].cast(l_type));\r\n",
        "        if (l_nullable!=r_nullable):\r\n",
        "          source_file.schema[l_name].nullable = l_nullable;\r\n",
        "      source_file = source_file.withColumn(l_name, lit(None).cast(l_type));\r\n",
        "      source_file.schema[l_name].nullable = l_nullable;\r\n",
        "      print(\"For the File-->\",file_number,\",the missing columns w.r.t. standard_schema :\",(l_name,l_type))\r\n",
        "    source_file=source_file.select(list(left_types.keys()));\r\n",
        "    for r_name, r_type ,r_nullable in right_fields.difference(left_fields):\r\n",
        "      print(\"For the File-->\",file_number,\",the extra columns w.r.t. standard_schema :\",(r_name,r_type));\r\n",
        "    if(right_fields==left_fields):\r\n",
        "      print(\"For the File-->\",file_number,\",the schema is exactly matched with standard_schema\");\r\n",
        "    else:\r\n",
        "      source_file.coalesce(1).write.mode('overwrite').option('header','true').csv('/content/Corrupted Files Storage/'+str(file_number)+'.csv')\r\n",
        "    return source_file;"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOxEikd4YPBF"
      },
      "source": [
        "# **Make the Schema Standard**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQCF4WDxXE1K"
      },
      "source": [
        "standard_schema=df3.schema"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ffrtzj4JYSt5"
      },
      "source": [
        "# Make the Schema standard & union those "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FnWyVQbEXZn6",
        "outputId": "c09e00c2-4e40-4965-f7cc-8c1a85475b71"
      },
      "source": [
        "df1=harmonize_schemas(standard_schema, df1);\r\n",
        "df2=harmonize_schemas(standard_schema, df2);\r\n",
        "df1.unionByName(df2).unionByName(df3).show()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "For the File--> 1 ,the missing columns w.r.t. standard_schema : ('C', LongType)\n",
            "For the File--> 1 ,the missing columns w.r.t. standard_schema : ('A_D', LongType)\n",
            "For the File--> 2 ,the missing columns w.r.t. standard_schema : ('A_D', LongType)\n",
            "+---+----+----+\n",
            "|A_B| A_D|   C|\n",
            "+---+----+----+\n",
            "|  1|null|null|\n",
            "|  1|null|   2|\n",
            "|  1|   3|   2|\n",
            "+---+----+----+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
